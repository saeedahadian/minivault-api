# MiniVault API

A lightweight, local REST API that simulates prompt/response generation with thoughtful engineering and clean design.

## Features

- âœ… RESTful API with `POST /generate` endpoint
- ğŸ“ Automatic JSONL logging with nanosecond precision
- ğŸš€ Token-by-token streaming support
- ğŸ›¡ï¸ Built-in rate limiting
- ğŸ“Š Hidden health metrics endpoint
- ğŸ¯ CLI testing tool with benchmarking
- ğŸ¤– AI-assisted development practices

## Quick Start

### Installation

```bash
# Clone the repository
cd minivault-api

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Running the API

```bash
# Start the server
python app.py

# Or with auto-reload for development
uvicorn app:app --reload
```

The API will be available at `http://localhost:8000`

### Basic Usage

#### Generate Response

```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello, MiniVault!"}'
```

#### Stream Response (SSE)

```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Tell me a story", "stream": true}'
```

## CLI Tool

The included CLI tool makes testing easier:

```bash
# Make it executable
chmod +x cli.py

# Generate response
./cli.py generate -p "Your prompt here"

# Stream response
./cli.py stream -p "Tell me about AI"

# Check health status
./cli.py health

# Run benchmark
./cli.py benchmark -c 100
```

## API Endpoints

### POST /generate
Generate a response for a given prompt. Supports both regular and streaming responses.

**Request:**
```json
{
  "prompt": "Your prompt text",
  "stream": false  // Optional: set to true for SSE streaming
}
```

**Response (Regular):**
```json
{
  "response": "Generated response text",
  "usage": {
    "prompt_tokens": 5,
    "completion_tokens": 10,
    "total_tokens": 15
  },
  "generated_at": "2024-01-10T12:00:00"
}
```

**Response (Streaming):**
Server-Sent Events format:
```
data: {"token": "Hello", "index": 0}
data: {"token": " world", "index": 1, "usage": {...}}
data: [DONE]
```

### GET /health (Hidden)
Returns system health metrics including uptime, request count, and average response time.

## Design Choices

1. **FastAPI Framework**: Modern, fast, with automatic validation and documentation
2. **Unified Endpoint**: Single `/generate` endpoint handles both streaming and non-streaming
3. **SSE Streaming**: Standards-compliant Server-Sent Events instead of NDJSON
4. **Usage Tracking**: Token counting for prompt and completion (mimics real AI APIs)
5. **Async Everything**: Non-blocking I/O for better performance
6. **Queue-based Logging**: Ensures logging doesn't block request handling
7. **Graceful Shutdown**: Properly handles SIGTERM/SIGINT signals

## Project Structure

```
minivault-api/
â”œâ”€â”€ app.py              # Main FastAPI application
â”œâ”€â”€ models.py           # Pydantic models with full type hints
â”œâ”€â”€ logger.py           # Async JSONL logger implementation
â”œâ”€â”€ cli.py              # Rich CLI tool for testing
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ logs/              # Directory for JSONL logs
â”‚   â””â”€â”€ log.jsonl      # Append-only log file
â”œâ”€â”€ DEVELOPMENT.md     # Development insights and AI collaboration notes
â””â”€â”€ README.md          # This file
```

## Future Improvements

- Integration with local LLMs (Ollama, Hugging Face)
- Prometheus metrics export
- Request ID tracking for better debugging
- Configuration via environment variables
- More sophisticated token counting (using actual tokenizers)

## Notes

This API includes several easter eggs and thoughtful touches. Try sending "What model are you?" as a prompt! ğŸ‰

---

Built with â¤ï¸ and AI assistance